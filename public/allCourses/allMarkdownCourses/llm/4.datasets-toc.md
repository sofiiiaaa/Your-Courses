

<!-- toc -->

- [Dataset](#dataset)
  * [Instruction-tuning dataset](#instruction-tuning-dataset)
    + [Model instruction format](#model-instruction-format)
  * [Basic dataset for instruction-tuning](#basic-dataset-for-instruction-tuning)
  * [Train on completions only](#train-on-completions-only)
    + [DataCollatorForCompletionOnlyLM](#datacollatorforcompletiononlylm)
  * [Packing dataset](#packing-dataset)

<!-- tocstop -->

# Dataset

Creating a dataset for fine-tuning a Large Language Model (LLM) involves several key steps and considerations to ensure the data is both relevant and of high quality.
For Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), different datasets are required to facilitate the training process effectively. Here's an overview of the datasets needed for each:

**Supervised Fine-Tuning (SFT)**
Training Dataset: This dataset is used to fine-tune the model on specific tasks or domains. It typically consists of input-output pairs or document chunks, depending on the task. For example, in a conversational context, the dataset might include prompts and their corresponding completions. The dataset should be formatted in a way that the SFTTrainer can process, such as using a formatting function to structure the data appropriately 

```
| text |
|------|
| human: hello \n bot: hi nice to meet you |
| human: how are you \n bot: I am fine |
| human: What is your name? \n bot: My name is Mary |
| human: Which is the best programming language? \n bot: Python |
```

**Direct Preference Optimization (DPO)**
Preference Data: DPO requires preference data, which is a collection of responses to prompts along with specified preference pairs indicating which response is preferred over another. This data is used to train the model to generate responses that align with human preferences. The dataset should include:

```
| prompt | text | rejected_text |
|--------|------|---------------|
| hello | hi nice to meet you | leave me alone |
| how are you | I am fine | I am not fine |
| What is your name? | My name is Mary | Whats it to you? |
| What is your name? | My name is Mary | I dont have a name |
```


### Model instruction format
Different models might be trained to understand and generate responses based on specific formatting cues in the input data. These cues help the model to distinguish between different parts of the input, such as instructions, user inputs, or system responses. Adhering to these formats is crucial for achieving optimal performance when fine-tuning or deploying these models.

For example, the Mistral model uses a specific instruction format. They can be `special_tokens` or can be also normal token (wrt to the tokenizer). a comprehensive list of special tokens can be found here https://github.com/unslothai/unsloth/blob/main/unsloth/chat_templates.py.
## MLM Completion vs MLM on all tokens

When fine-tuning a model, you can choose to train it using Masked Language Modeling (MLM) on all tokens or only on the completion tokens. The choice depends on the specific requirements of the task and the dataset.


### DataCollatorForCompletionOnlyLM
This collator is specifically designed for scenarios where the model is expected to generate completions to prompts. It is optimized for tasks where the goal is to produce a continuation or completion of a given input, such as in conversational contexts or when generating responses to prompts. The use of this collator suggests a focus on training the model to generate responses rather than predicting tokens within the prompt itself.
By focusing on completions, this collator might be more efficient for tasks that require generating longer sequences or when the prompts are relatively short. It could potentially lead to more targeted training, where the model learns to generate responses that closely match the desired outputs provided in the training dataset.

You can use the `DataCollatorForCompletionOnlyLM` to train your model on the generated prompts only. Note that this works only in the case when `packing=False`. To instantiate that collator for instruction data, pass a response template and the tokenizer.

```python
from trl import DataCollatorForCompletionOnlyLM

collator = DataCollatorForCompletionOnlyLM(
    instruction_template="[INST]",
    response_template="[/INST]",
    tokenizer=tokenizer
  )

trainer = SFTTrainer(
    ...,
    data_collator=collator
)
```

## Packing dataset

 SFTTrainer supports example packing, where multiple short examples are packed in the same input sequence to increase training efficiency. Packing involves concatenating multiple tokenized sequences together into a single input, referred to as a 'pack'. This technique is particularly useful for datasets with heavily skewed length distributions towards the shorter side, as transformer models typically receive fixed-sized inputs. By packing sequences together, padding is eliminated, computational waste is mitigated, and the benefits of a model represented as a static graph with constant-sized inputs are maintained. This allows for more sequences to be processed per batch, with multiple sequences within a pack being processed in parallel on a token level, effectively increasing batch size with minimal overhead and bringing significant throughput benefits.

 ```py
 trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    packing=True
)
```


## Sources:
- https://github.com/lmmlzn/awesome-llms-datasets
- https://github.com/huggingface/trl/issues/632#issuecomment-1911925051
- https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only
- https://github.com/huggingface/trl/issues/632#issuecomment-1807840592
